{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "llgetzycue4pc4lqzwof",
   "authorId": "4227028274930",
   "authorName": "KAMESHS",
   "authorEmail": "kamesh.sampath@snowflake.com",
   "sessionId": "38f5d474-5e68-494c-ae8c-13f987693b0d",
   "lastEditTime": 1760689507712
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "intro",
    "collapsed": false
   },
   "source": "# Getting Started with Snowflake Openflow Kafka Connector\n\nCompanion code for the [Snowflake Openflow Kafka Connector Quickstart](https://quickstarts.snowflake.com/guide/getting_started_with_openflow_kafka_connector/index.html).\n\n## Overview\n\nThis quickstart demonstrates how to build a real-time streaming pipeline from Apache Kafka to Snowflake using the Openflow Kafka Connector. You'll learn how to:\n\n- Set up a Kafka topic for application log streaming\n- Configure Snowflake objects (database, schema, tables, network rules)\n- Deploy Openflow SPCS runtime\n- Configure the Kafka connector in Openflow Canvas\n- Stream real-time logs from Kafka to Snowflake\n- Perform powerful SQL analytics on streaming log data\n"
  },
  {
   "cell_type": "markdown",
   "id": "4da2f77c-7669-4b17-be91-1fad096f7a23",
   "metadata": {
    "name": "md_setup",
    "collapsed": false
   },
   "source": "## Database Setup\nThe following cell sets up all required Snowflake objects for the Kafka log streaming demo. Run this BEFORE configuring the Openflow connector"
  },
  {
   "cell_type": "code",
   "id": "959d36b5-f443-43fb-8167-57a7de3d996e",
   "metadata": {
    "language": "sql",
    "name": "setup"
   },
   "outputs": [],
   "source": "\nUSE ROLE ACCOUNTADMIN;\n\n-- Step 1: Create Role and Database\n-- ----------------------------------------------------------------------------\n\n-- Create runtime role (reuse if coming from SPCS quickstart)\nCREATE ROLE IF NOT EXISTS QUICKSTART_ROLE;\n\n-- Create database for Kafka streaming data\nCREATE DATABASE IF NOT EXISTS QUICKSTART_KAFKA_CONNECTOR_DB;\n\n-- Create warehouse for data processing and queries\nCREATE WAREHOUSE IF NOT EXISTS QUICKSTART_KAFKA_CONNECTOR_WH\n  WAREHOUSE_SIZE = XSMALL\n  AUTO_SUSPEND = 60\n  AUTO_RESUME = TRUE\n  INITIALLY_SUSPENDED = TRUE;\n\n-- Grant privileges to runtime role\nGRANT OWNERSHIP ON DATABASE QUICKSTART_KAFKA_CONNECTOR_DB TO ROLE QUICKSTART_ROLE;\nGRANT OWNERSHIP ON SCHEMA QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC TO ROLE QUICKSTART_ROLE;\nGRANT USAGE ON WAREHOUSE QUICKSTART_KAFKA_CONNECTOR_WH TO ROLE QUICKSTART_ROLE;\n\n-- Grant runtime role to Openflow admin\nGRANT ROLE QUICKSTART_ROLE TO ROLE OPENFLOW_ADMIN;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0e2b9ecf-bff9-4a2d-9ed9-dca15621950c",
   "metadata": {
    "name": "md_network_rule",
    "collapsed": false
   },
   "source": "## Create Network Rule\n\nCreate network rule to allow Openflow runtime to access the Kafka clusters\n\n> **IMPORTANT**: Replace with your Kafka broker endpoint(s)\n> This quickstart works with any Kafka service:\n> - GCP Managed Kafka:    `34.123.45.67:9092(public IP)`\n> - AWS MSK:              `b-1.mycluster.kafka.us-east-1.amazonaws.com:9092`\n> - Confluent Cloud:      `pkc-xxxxx.us-east-1.aws.confluent.cloud:9092`\n> - Azure Event Hubs:     `myeventhub.servicebus.windows.net:9093`\n> - Self-hosted:          `kafka.mycompany.com:9092`\n>\n> **NOTE**: Ensure network connectivity and firewall rules allow Snowflake access\n> For multiple brokers (recommended), include all broker endpoints in VALUE_LIST"
  },
  {
   "cell_type": "code",
   "id": "38f3fd90-3410-4add-b8c3-84553ca60464",
   "metadata": {
    "language": "sql",
    "name": "create_network_rule"
   },
   "outputs": [],
   "source": "USE ROLE QUICKSTART_ROLE;\nUSE DATABASE QUICKSTART_KAFKA_CONNECTOR_DB;\n\n-- Create schema for network rules\nCREATE SCHEMA IF NOT EXISTS QUICKSTART_KAFKA_CONNECTOR_DB.NETWORKS;\n\nCREATE OR REPLACE NETWORK RULE QUICKSTART_KAFKA_CONNECTOR_DB.NETWORKS.kafka_network_rule\n  MODE = EGRESS\n  TYPE = HOST_PORT\n  VALUE_LIST = (\n     -- use wild card for similar domin patterns or\n    -- add all the bootstrap servers\n);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72edf0b2-2413-4b7c-9853-8102600662e4",
   "metadata": {
    "language": "sql",
    "name": "describe_network_rule"
   },
   "outputs": [],
   "source": "DESC NETWORK RULE QUICKSTART_KAFKA_CONNECTOR_DB.NETWORKS.kafka_network_rule;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71efb410-760d-42a9-9a0c-444dee0dda18",
   "metadata": {
    "name": "external_access_integration",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "94fcfa66-f113-44ad-8496-7f0492319e2a",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Create External Access Integration(EAI)\n\nCreate an EAI to that can be associated with the Openflow runtime"
  },
  {
   "cell_type": "code",
   "id": "c3a7d2a6-a8d3-4d4d-afeb-3ef183639a69",
   "metadata": {
    "language": "sql",
    "name": "create_eai"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\n\nCREATE OR REPLACE EXTERNAL ACCESS INTEGRATION quickstart_kafka_connector_access\n  ALLOWED_NETWORK_RULES = (\n    QUICKSTART_KAFKA_CONNECTOR_DB.NETWORKS.kafka_network_rule\n  )\n  ENABLED = TRUE\n  COMMENT = 'Openflow SPCS runtime access for Kafka connector';\n\n-- Grant usage to runtime role\nGRANT USAGE ON INTEGRATION quickstart_kafka_connector_access TO ROLE OPENFLOW_ADMIN;\n-- Grant usage to runtime role\nGRANT USAGE ON INTEGRATION quickstart_kafka_connector_access TO ROLE QUICKSTART_ROLE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3cc803e5-d383-432d-8fd4-4c244a1f9e19",
   "metadata": {
    "language": "sql",
    "name": "describe_eai"
   },
   "outputs": [],
   "source": "DESCRIBE EXTERNAL ACCESS INTEGRATION quickstart_kafka_connector_access;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e15ebbf-daf9-4727-ac17-65fe2a61086f",
   "metadata": {
    "name": "create_runtime",
    "collapsed": false
   },
   "source": "## Create Runtime\nCreate quickstart runtime as described in the Quickstart guide. Once the connector is configured and all running, you can use the other cells to verify the setup"
  },
  {
   "cell_type": "markdown",
   "id": "cb6becf4-04a8-4933-939d-e1ddd94236b1",
   "metadata": {
    "name": "verify_ingestion",
    "collapsed": false
   },
   "source": "## Verify Ingestion\n\nLets verify the ingestion that happens form our Kafka topic's to `\"APPLICATION-LOGS\"` tables\n\n>**IMPORTANT**:\n>\n> Ensure that you have triggered Apache Kafka stream ingestion following instructions in the [Chapter 9](http://quickstarts.snowflake.com/guide/getting-started-with-openflow-kafka-connector/index.html?index=..%2F..index#9)"
  },
  {
   "cell_type": "code",
   "id": "b93a73d2-ff16-4a10-8d9f-930b2ef37d14",
   "metadata": {
    "language": "sql",
    "name": "verify_set_ingestion_context"
   },
   "outputs": [],
   "source": "USE ROLE QUICKSTART_ROLE;\nUSE DATABASE QUICKSTART_KAFKA_CONNECTOR_DB;\nUSE SCHEMA PUBLIC;\nUSE WAREHOUSE QUICKSTART_KAFKA_CONNECTOR_WH;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d955eb6c-1d7f-4e85-9a1f-a9170f195eb1",
   "metadata": {
    "language": "sql",
    "name": "list_all_tables"
   },
   "outputs": [],
   "source": "SHOW TABLES;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62abc8c1-fb4c-4703-a752-881902507432",
   "metadata": {
    "name": "phase_1_inital_schema",
    "collapsed": false
   },
   "source": "### Verify Initial Schema\nLets verify the table that was created and data thats ingested"
  },
  {
   "cell_type": "markdown",
   "id": "092ae38b-b516-440b-bf3f-75747c637aa9",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "Expected: ~50+ records, the messages that were sent to `application-logs` topic"
  },
  {
   "cell_type": "code",
   "id": "26c69f99-391e-49b9-8496-f8d0962f0c38",
   "metadata": {
    "language": "sql",
    "name": "count_ingested_records"
   },
   "outputs": [],
   "source": "SELECT COUNT(*) as TOTAL_RECORDS FROM \"APPLICATION-LOGS\";",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bfb72fdc-ad6e-4bfe-a065-2525a6a9db2a",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "Let us inspect the initial Capture Initial Schema, it should have *11* columns."
  },
  {
   "cell_type": "code",
   "id": "0c4f3f0e-518d-446b-9259-d877220a40fe",
   "metadata": {
    "language": "sql",
    "name": "base_schema"
   },
   "outputs": [],
   "source": "SELECT \n  COLUMN_NAME, \n  DATA_TYPE\nFROM QUICKSTART_KAFKA_CONNECTOR_DB.INFORMATION_SCHEMA.COLUMNS \nWHERE TABLE_NAME ILIKE 'APPLICATION-LOGS'\nORDER BY COLUMN_NAME;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00bbf29e-0d95-4c5d-9278-09f847f7bfcf",
   "metadata": {
    "language": "python",
    "name": "print_base_columns"
   },
   "outputs": [],
   "source": "# Convert SQL result to pandas DataFrame and extract column names\ncolumn_names_df = base_schema.to_pandas()['COLUMN_NAME'].sort_values()\n\n# Display the sorted column names\nprint(f\"Total Number of Columns:{len(column_names_df)}\")\nprint(column_names_df.to_string(index=False))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f279acb-2a63-4a1b-b792-856465b3efe1",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "#### Query Base Schema Data"
  },
  {
   "cell_type": "code",
   "id": "1a582a45-f30d-469e-92b3-8397e4ba9025",
   "metadata": {
    "language": "sql",
    "name": "sample_data"
   },
   "outputs": [],
   "source": "SELECT \n  TIMESTAMP,\n  LEVEL,\n  SERVICE,\n  MESSAGE,\n  STATUS_CODE,\n  DURATION_MS\nFROM \"APPLICATION-LOGS\"\nORDER BY TIMESTAMP DESC\nLIMIT 10;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "466e7e08-1aff-41e0-add3-f4a93142e80f",
   "metadata": {
    "language": "sql",
    "name": "log_level_distribution"
   },
   "outputs": [],
   "source": "SELECT \n  LEVEL,\n  COUNT(*) as LOG_COUNT\nFROM \"APPLICATION-LOGS\"\nGROUP BY LEVEL\nORDER BY LOG_COUNT DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9ab2b5fd-7498-471e-adf9-9b0e9d5fe025",
   "metadata": {
    "language": "sql",
    "name": "error_analysis"
   },
   "outputs": [],
   "source": "SELECT \n  TIMESTAMP,\n  SERVICE,\n  MESSAGE,\n  ERROR,\n  STATUS_CODE\nFROM \"APPLICATION-LOGS\"\nWHERE ERROR IS NOT NULL\nORDER BY TIMESTAMP DESC\nLIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2912425c-8ace-48be-aa1b-29e407ef20e6",
   "metadata": {
    "language": "sql",
    "name": "service_health_summary"
   },
   "outputs": [],
   "source": "SELECT \n  SERVICE,\n  COUNT(*) as TOTAL_LOGS,\n  SUM(CASE WHEN LEVEL = 'ERROR' THEN 1 ELSE 0 END) as ERROR_COUNT,\n  SUM(CASE WHEN LEVEL = 'WARN' THEN 1 ELSE 0 END) as WARN_COUNT,\n  ROUND(AVG(DURATION_MS), 2) as AVG_DURATION_MS\nFROM \"APPLICATION-LOGS\"\nGROUP BY SERVICE\nORDER BY ERROR_COUNT DESC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "704a5e0f-3423-4142-874a-da8df022e51d",
   "metadata": {
    "name": "verify_evolve_schema",
    "collapsed": false
   },
   "source": "### Verify Evolved Schema\nLets verify how the `\"APPLICATION-LOGS\"` table evolves when we get messges with extra columns\n\n>**IMPORTANT**:\n>\n> Ensure that you have triggered Apache Kafka stream ingestion following instructions in the [Chapter 10](http://quickstarts.snowflake.com/guide/guide/getting-started-with-openflow-kafka-connector/index.html?index=..%2F..index#10)"
  },
  {
   "cell_type": "markdown",
   "id": "3ed605b4-7c54-48cf-b160-1d88f81082aa",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "Compare Schemas - **NEW** columns appeared, the table should have `11(base) + 26(evolved) = 37columns`."
  },
  {
   "cell_type": "code",
   "id": "1ff669ed-c916-4155-808c-5b553d3078f1",
   "metadata": {
    "language": "sql",
    "name": "evolved_columns"
   },
   "outputs": [],
   "source": "SELECT \n  COLUMN_NAME, \n  DATA_TYPE\nFROM QUICKSTART_KAFKA_CONNECTOR_DB.INFORMATION_SCHEMA.COLUMNS \nWHERE TABLE_NAME ILIKE 'APPLICATION-LOGS'\nORDER BY COLUMN_NAME;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49c4b890-5ee9-4469-87b0-d43bca6245ea",
   "metadata": {
    "language": "python",
    "name": "print_evolved_columns",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Convert SQL result to pandas DataFrame and extract column names\ncolumn_names_df = evolved_columns.to_pandas()['COLUMN_NAME'].sort_values()\n\n# Display the sorted column names\nprint(f\"Total Number of Columns:{len(column_names_df)}\")\nprint(column_names_df.to_string(index=False))\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22e7bd28-f392-404a-a030-68449d583b8f",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "#### Query Evolved Schema Fields"
  },
  {
   "cell_type": "code",
   "id": "16755fc5-a637-4215-8249-29b0df5c0c80",
   "metadata": {
    "language": "sql",
    "name": "region_fields"
   },
   "outputs": [],
   "source": "SELECT \n  TIMESTAMP,\n  SERVICE,\n  MESSAGE,\n  REGION,\n  TRACE_ID\nFROM \"APPLICATION-LOGS\"\nWHERE REGION IS NOT NULL\nORDER BY TIMESTAMP DESC\nLIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1cc4041-8d5c-4f7f-bb37-f8ebc04f99e3",
   "metadata": {
    "language": "sql",
    "name": "new_authentication_field"
   },
   "outputs": [],
   "source": "SELECT \n  TIMESTAMP,\n  SERVICE,\n  USER_ID,\n  AUTH_METHOD,\n  PROVIDER,\n  REGION\nFROM \"APPLICATION-LOGS\"\nWHERE AUTH_METHOD IS NOT NULL\nORDER BY TIMESTAMP DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9288936-c77b-4886-a62f-29d32ccdf201",
   "metadata": {
    "language": "sql",
    "name": "payment_logs_with_currency"
   },
   "outputs": [],
   "source": "SELECT \n  TIMESTAMP,\n  SERVICE,\n  AMOUNT,\n  CURRENCY,\n  PAYMENT_METHOD,\n  USER_ID,\n  REGION\nFROM \"APPLICATION-LOGS\"\nWHERE CURRENCY IS NOT NULL\nORDER BY TIMESTAMP DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7890216-ed2a-4c99-a69c-33ca0bd5c006",
   "metadata": {
    "language": "sql",
    "name": "file_uploads_with_metadata"
   },
   "outputs": [],
   "source": "SELECT \n  TIMESTAMP,\n  SERVICE,\n  MESSAGE,\n  FILE_SIZE_BYTES,\n  CONTENT_TYPE,\n  REGION\nFROM \"APPLICATION-LOGS\"\nWHERE FILE_SIZE_BYTES IS NOT NULL\nORDER BY TIMESTAMP DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2256b65-3211-43d7-bb0e-69cb64d46a23",
   "metadata": {
    "language": "sql",
    "name": "system_metrics"
   },
   "outputs": [],
   "source": "SELECT \n  TIMESTAMP,\n  SERVICE,\n  MESSAGE,\n  MEMORY_PERCENT,\n  AVAILABLE_MB,\n  DISK_USAGE_PERCENT,\n  AVAILABLE_GB,\n  REGION\nFROM \"APPLICATION-LOGS\"\nWHERE MEMORY_PERCENT IS NOT NULL \n   OR DISK_USAGE_PERCENT IS NOT NULL\nORDER BY TIMESTAMP DESC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "025d7496-615e-4133-a328-f37c37468479",
   "metadata": {
    "name": "analytics",
    "collapsed": false
   },
   "source": "## Analytics on Log Data\nNow that logs are streaming into Snowflake, let's perform powerful analytics that would be difficult or expensive in traditional log platforms."
  },
  {
   "cell_type": "code",
   "id": "b72cfc7a-44a5-4fc4-a3fe-54e16ab1137c",
   "metadata": {
    "language": "python",
    "name": "count_by_log_level",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport altair as alt\nfrom snowflake.snowpark.context import get_active_session\n\n# Get active session\nsession = get_active_session()\n\n# Execute SQL query\nsql = \"\"\"\nSELECT \n    LEVEL as LOG_LEVEL,\n    COUNT(*) as EVENT_COUNT,\n    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as PERCENTAGE\nFROM \"APPLICATION-LOGS\"\nGROUP BY LOG_LEVEL\nORDER BY EVENT_COUNT DESC\n\"\"\"\n\n# Convert results to DataFrame\ndf = session.sql(sql).to_pandas()\n\n# Create visualization\nst.header(\"Log Level Distribution\")\n\n# Create metrics for total events\ntotal_events = df['EVENT_COUNT'].sum()\nst.metric(\"Total Log Events\", f\"{total_events:,}\")\n\n# Create bar chart\nchart = alt.Chart(df).mark_bar().encode(\n    x=alt.X('LOG_LEVEL:N', title='Log Level'),\n    y=alt.Y('EVENT_COUNT:Q', title='Number of Events'),\n    color=alt.Color('LOG_LEVEL:N', legend=None),\n    tooltip=[\n        alt.Tooltip('LOG_LEVEL:N', title='Level'),\n        alt.Tooltip('EVENT_COUNT:Q', title='Count'),\n        alt.Tooltip('PERCENTAGE:Q', title='Percentage', format='.2f')\n    ]\n).properties(\n    width=600,\n    height=400\n)\n\nst.altair_chart(chart, use_container_width=True)\n\n# Display data table\nst.dataframe(df, hide_index=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f989c290-ff97-40be-8201-304a59f29017",
   "metadata": {
    "language": "python",
    "name": "top_10_common_error_messages",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport altair as alt\nfrom snowflake.snowpark.context import get_active_session\n\n# Get active session\nsession = get_active_session()\n\n# Execute SQL query\nsql = \"\"\"\nSELECT \n    MESSAGE as ERROR_MESSAGE,\n    SERVICE,\n    COUNT(*) as ERROR_COUNT\nFROM \"APPLICATION-LOGS\"\nWHERE LEVEL ILIKE '%ERROR%'\nGROUP BY ERROR_MESSAGE, SERVICE\nORDER BY ERROR_COUNT DESC\nLIMIT 10\n\"\"\"\n\n# Convert results to DataFrame\ndf = session.sql(sql).to_pandas()\n\n# Create visualization\nst.header(\"Top Error Messages by Service\")\n\n# Create summary metrics\ntotal_errors = df['ERROR_COUNT'].sum()\nunique_services = df['SERVICE'].nunique()\nst.metric(\"Total Errors\", total_errors)\nst.metric(\"Affected Services\", unique_services)\n\n# Create bar chart\nchart = alt.Chart(df).mark_bar().encode(\n    y=alt.Y('ERROR_MESSAGE:N', \n            sort='-x',\n            title='Error Message',\n            axis=alt.Axis(labelLimit=250)),\n    x=alt.X('ERROR_COUNT:Q', title='Number of Occurrences'),\n    color=alt.Color('SERVICE:N', title='Service'),\n    tooltip=[\n        alt.Tooltip('ERROR_MESSAGE:N', title='Error'),\n        alt.Tooltip('SERVICE:N', title='Service'),\n        alt.Tooltip('ERROR_COUNT:Q', title='Count')\n    ]\n).properties(\n    height=400\n)\n\nst.altair_chart(chart, use_container_width=True)\n\n# Display detailed data\nwith st.expander(\"View Detailed Data\"):\n    st.dataframe(df, hide_index=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b14b8f0-ad0d-4db9-b563-8dc57771607c",
   "metadata": {
    "language": "python",
    "name": "service_health_overview",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport altair as alt\nfrom snowflake.snowpark.context import get_active_session\n\n# Get active session\nsession = get_active_session()\n\n# Execute SQL query\nsql = \"\"\"\nSELECT \n    SERVICE as SERVICE_NAME,\n    COUNT(*) as TOTAL_EVENTS,\n    SUM(CASE WHEN LEVEL ILIKE '%ERROR%' THEN 1 ELSE 0 END) as ERROR_COUNT,\n    SUM(CASE WHEN LEVEL ILIKE '%WARN%' THEN 1 ELSE 0 END) as WARN_COUNT,\n    ROUND(ERROR_COUNT * 100.0 / NULLIF(TOTAL_EVENTS, 0), 2) as ERROR_RATE_PCT\nFROM \"APPLICATION-LOGS\"\nGROUP BY SERVICE_NAME\nORDER BY ERROR_RATE_PCT DESC\n\"\"\"\n\n# Convert results to DataFrame\ndf = session.sql(sql).to_pandas()\n\n# Create visualization\nst.header(\"Service Health Overview\")\n\n# Create summary metrics\ncol1, col2, col3, col4 = st.columns(4)\nwith col1:\n    st.metric(\"Total Services\", len(df))\nwith col2:\n    st.metric(\"Total Events\", df['TOTAL_EVENTS'].sum())\nwith col3:\n    st.metric(\"Total Errors\", df['ERROR_COUNT'].sum())\nwith col4:\n    st.metric(\"Total Warnings\", df['WARN_COUNT'].sum())\n\n# Create main chart\nchart = alt.Chart(df).mark_bar().encode(\n    x=alt.X('SERVICE_NAME:N', title='Service'),\n    y=alt.Y('TOTAL_EVENTS:Q', title='Number of Events'),\n    color=alt.Color('ERROR_RATE_PCT:Q', \n                   scale=alt.Scale(scheme='redyellowgreen', reverse=True),\n                   title='Error Rate (%)'),\n    tooltip=[\n        alt.Tooltip('SERVICE_NAME:N', title='Service'),\n        alt.Tooltip('TOTAL_EVENTS:Q', title='Total Events'),\n        alt.Tooltip('ERROR_COUNT:Q', title='Errors'),\n        alt.Tooltip('WARN_COUNT:Q', title='Warnings'),\n        alt.Tooltip('ERROR_RATE_PCT:Q', title='Error Rate %', format='.2f')\n    ]\n).properties(\n    height=400\n)\n\nst.altair_chart(chart, use_container_width=True)\n\n# Display detailed data\nwith st.expander(\"View Detailed Data\"):\n    st.dataframe(df, hide_index=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7bd6c570-5ab5-42dd-969d-51dddfe87fd2",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "### Time-Series Analysis"
  },
  {
   "cell_type": "code",
   "id": "03dcb63a-77ad-4ae9-b297-f79ed5f79b80",
   "metadata": {
    "language": "python",
    "name": "events_per_minute",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport altair as alt\nfrom snowflake.snowpark.context import get_active_session\n\n# Get active session\nsession = get_active_session()\n\n# Execute SQL query\nsql = \"\"\"\nSELECT \n    DATE_TRUNC('minute', TIMESTAMP::TIMESTAMP) as TIME_BUCKET,\n    COUNT(*) as EVENTS_PER_MINUTE,\n    SUM(CASE WHEN LEVEL ILIKE '%ERROR%' THEN 1 ELSE 0 END) as ERRORS_PER_MINUTE\nFROM \"APPLICATION-LOGS\"\nWHERE TIMESTAMP::TIMESTAMP >= DATEADD('hour', -1, CURRENT_TIMESTAMP())\nGROUP BY TIME_BUCKET\nORDER BY TIME_BUCKET DESC\n\"\"\"\n\n# Convert results to DataFrame\ndf = session.sql(sql).to_pandas()\n\n# Create visualization\nst.header(\"Log Events Time Series Analysis\")\n\n# Create summary metrics\ncol1, col2, col3 = st.columns(3)\nwith col1:\n    st.metric(\"Total Events\", df['EVENTS_PER_MINUTE'].sum())\nwith col2:\n    st.metric(\"Total Errors\", df['ERRORS_PER_MINUTE'].sum())\nwith col3:\n    error_rate = round(df['ERRORS_PER_MINUTE'].sum() * 100 / df['EVENTS_PER_MINUTE'].sum(), 2) if df['EVENTS_PER_MINUTE'].sum() > 0 else 0\n    st.metric(\"Error Rate\", f\"{error_rate}%\")\n\n# Create time series chart\nbase = alt.Chart(df).encode(\n    x=alt.X('TIME_BUCKET:T', title='Time')\n)\n\nevents_line = base.mark_line(color='blue').encode(\n    y=alt.Y('EVENTS_PER_MINUTE:Q', title='Events per Minute'),\n    tooltip=[\n        alt.Tooltip('TIME_BUCKET:T', title='Time'),\n        alt.Tooltip('EVENTS_PER_MINUTE:Q', title='Events'),\n        alt.Tooltip('ERRORS_PER_MINUTE:Q', title='Errors')\n    ]\n)\n\nerrors_line = base.mark_line(color='red').encode(\n    y=alt.Y('ERRORS_PER_MINUTE:Q', title='Errors per Minute'),\n    tooltip=[\n        alt.Tooltip('TIME_BUCKET:T', title='Time'),\n        alt.Tooltip('EVENTS_PER_MINUTE:Q', title='Events'),\n        alt.Tooltip('ERRORS_PER_MINUTE:Q', title='Errors')\n    ]\n)\n\nchart = alt.layer(events_line, errors_line).resolve_scale(\n    y='independent'\n).properties(\n    height=400\n)\n\nst.altair_chart(chart, use_container_width=True)\n\n# Display detailed data\nwith st.expander(\"View Detailed Data\"):\n    st.dataframe(df, hide_index=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79dd8252-ad22-4584-a353-eceed93c7f65",
   "metadata": {
    "language": "python",
    "name": "events_by_hour",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport altair as alt\nfrom snowflake.snowpark.context import get_active_session\n\n# Get active session\nsession = get_active_session()\n\n# Execute SQL query\nsql = \"\"\"\nSELECT \n    HOUR(TIMESTAMP::TIMESTAMP) as HOUR_OF_DAY,\n    COUNT(*) as EVENT_COUNT,\n    AVG(DURATION_MS) as AVG_DURATION_MS\nFROM \"APPLICATION-LOGS\"\nWHERE TIMESTAMP::TIMESTAMP >= DATEADD('day', -1, CURRENT_TIMESTAMP())\nGROUP BY HOUR_OF_DAY\nORDER BY HOUR_OF_DAY\n\"\"\"\n\n# Convert results to DataFrame\ndf = session.sql(sql).to_pandas()\n\n# Create visualization\nst.header(\"Hourly Event Distribution\")\n\n# Create summary metrics\ncol1, col2 = st.columns(2)\nwith col1:\n    st.metric(\"Total Events\", df['EVENT_COUNT'].sum())\nwith col2:\n    avg_duration = round(df['AVG_DURATION_MS'].mean(), 2)\n    st.metric(\"Average Duration (ms)\", f\"{avg_duration:.2f}\")\n\n# Create dual-axis chart\nbase = alt.Chart(df).encode(\n    x=alt.X('HOUR_OF_DAY:Q', \n            axis=alt.Axis(title='Hour of Day', tickCount=24),\n            scale=alt.Scale(domain=[0, 23]))\n)\n\n# Event count bars\nbars = base.mark_bar().encode(\n    y=alt.Y('EVENT_COUNT:Q', title='Event Count'),\n    color=alt.value('#5276A7'),\n    tooltip=[\n        alt.Tooltip('HOUR_OF_DAY:Q', title='Hour'),\n        alt.Tooltip('EVENT_COUNT:Q', title='Events'),\n        alt.Tooltip('AVG_DURATION_MS:Q', title='Avg Duration (ms)', format='.2f')\n    ]\n)\n\n# Average duration line\nline = base.mark_line(color='red').encode(\n    y=alt.Y('AVG_DURATION_MS:Q', title='Average Duration (ms)')\n)\n\n# Combine charts\nchart = alt.layer(bars, line).resolve_scale(\n    y='independent'\n).properties(\n    height=400\n)\n\nst.altair_chart(chart, use_container_width=True)\n\n# Display detailed data\nwith st.expander(\"View Detailed Data\"):\n    st.dataframe(df, hide_index=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6701f994-1cb3-4010-a0f9-b4db8309e15c",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "## Performance Analytics"
  },
  {
   "cell_type": "code",
   "id": "fe5edbb7-8a1f-4ee8-8157-d8d0ac01e835",
   "metadata": {
    "language": "sql",
    "name": "slowest_requests",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT\n  SERVICE,\n  REQUEST_ID,\n  DURATION_MS,\n  MESSAGE,\n  TIMESTAMP AS REQUEST_TIME,\n  LEVEL,\n  STATUS_CODE\nFROM\n  QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.\"APPLICATION-LOGS\"\nWHERE\n  NOT DURATION_MS IS NULL\n  AND DURATION_MS > 0\nORDER BY\n  DURATION_MS DESC\nLIMIT\n  20;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17b6e4a4-0c12-4110-87ea-2deed287c33a",
   "metadata": {
    "language": "sql",
    "name": "averge_performance_by_service",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "SELECT\n  SERVICE,\n  COUNT(*) as TOTAL_REQUESTS,\n  ROUND(AVG(DURATION_MS), 2) as AVG_DURATION_MS,\n  MIN(DURATION_MS) as MIN_DURATION_MS,\n  MAX(DURATION_MS) as MAX_DURATION_MS,\n  SUM(\n    CASE\n      WHEN STATUS_CODE >= 400 THEN 1\n      ELSE 0\n    END\n  ) as ERROR_COUNT,\n  ROUND(\n    AVG(\n      CASE\n        WHEN STATUS_CODE >= 400 THEN 1\n        ELSE 0\n      END\n    ) * 100,\n    2\n  ) as ERROR_RATE_PCT\nFROM\n  QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.\"APPLICATION-LOGS\"\nWHERE\n  DURATION_MS IS NOT NULL\n  AND DURATION_MS > 0\nGROUP BY\n  SERVICE\nORDER BY\n  AVG_DURATION_MS DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb5ebcb9-868c-497a-8e12-83855d1b93d7",
   "metadata": {
    "language": "sql",
    "name": "error_correlation",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "WITH ERROR_WINDOWS AS (\n  SELECT\n    SERVICE,\n    CAST(TIMESTAMP AS TIMESTAMP) AS ERROR_TIME,\n    REQUEST_ID,\n    STATUS_CODE,\n    DURATION_MS,\n    DATE_TRUNC ('MINUTE', CAST(TIMESTAMP AS TIMESTAMP)) AS TIME_WINDOW\n    /* Create 5-minute time windows */\n  FROM\n    QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.\"APPLICATION-LOGS\"\n  WHERE\n    LEVEL ILIKE '%ERROR%'\n),\nSERVICE_PAIRS AS (\n  SELECT\n    a.SERVICE AS SERVICE_A,\n    b.SERVICE AS SERVICE_B,\n    a.TIME_WINDOW,\n    COUNT(DISTINCT a.REQUEST_ID) AS ERRORS_SERVICE_A,\n    COUNT(DISTINCT b.REQUEST_ID) AS ERRORS_SERVICE_B,\n    AVG(a.DURATION_MS) AS AVG_DURATION_A,\n    AVG(b.DURATION_MS) AS AVG_DURATION_B\n  FROM\n    ERROR_WINDOWS AS a\n    JOIN ERROR_WINDOWS AS b ON a.TIME_WINDOW = b.TIME_WINDOW\n    AND a.SERVICE < b.SERVICE\n    /* Avoid duplicate pairs */\n  GROUP BY\n    a.SERVICE,\n    b.SERVICE,\n    a.TIME_WINDOW\n  HAVING\n    ERRORS_SERVICE_A > 0\n    AND ERRORS_SERVICE_B > 0\n)\nSELECT\n  SERVICE_A,\n  SERVICE_B,\n  COUNT(*) AS CONCURRENT_ERROR_WINDOWS,\n  SUM(ERRORS_SERVICE_A) AS TOTAL_ERRORS_A,\n  SUM(ERRORS_SERVICE_B) AS TOTAL_ERRORS_B,\n  ROUND(AVG(AVG_DURATION_A), 2) AS AVG_DURATION_A,\n  ROUND(AVG(AVG_DURATION_B), 2) AS AVG_DURATION_B\nFROM\n  SERVICE_PAIRS\nGROUP BY\n  SERVICE_A,\n  SERVICE_B\nHAVING\n  CONCURRENT_ERROR_WINDOWS > 1\nORDER BY\n  CONCURRENT_ERROR_WINDOWS DESC,\n  TOTAL_ERRORS_A + TOTAL_ERRORS_B DESC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c0f0a4c8-217a-4cbc-a523-8ecb7fa31f13",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "## Using Cortex Search\nEnable natural language search over your log messages using Snowflake Cortex Search. This allows you to query logs using plain English through Snowflake Intelligence or programmatically."
  },
  {
   "cell_type": "code",
   "id": "c4d39882-4c45-45d5-b5ce-60812a6d79e4",
   "metadata": {
    "language": "sql",
    "name": "create_search_service"
   },
   "outputs": [],
   "source": "USE ROLE QUICKSTART_ROLE;\nUSE DATABASE QUICKSTART_KAFKA_CONNECTOR_DB;\nUSE SCHEMA PUBLIC;\nUSE WAREHOUSE QUICKSTART_KAFKA_CONNECTOR_WH;\n\nCREATE OR REPLACE CORTEX SEARCH SERVICE application_logs_search\n  ON MESSAGE\n  ATTRIBUTES LEVEL, SERVICE, ERROR, STATUS_CODE, MEMORY_PERCENT, DISK_USAGE_PERCENT, REGION\n  WAREHOUSE = QUICKSTART_KAFKA_CONNECTOR_WH\n  TARGET_LAG = '1 minute'\n  AS (\n    SELECT \n      MESSAGE,\n      LEVEL,\n      SERVICE,\n      ERROR,\n      STATUS_CODE,\n      TIMESTAMP,\n      REQUEST_ID,\n      HOST,\n      USER_ID,\n      MEMORY_PERCENT::NUMBER as MEMORY_PERCENT,\n      AVAILABLE_MB::NUMBER as AVAILABLE_MB,\n      DISK_USAGE_PERCENT::NUMBER as DISK_USAGE_PERCENT,\n      AVAILABLE_GB::NUMBER as AVAILABLE_GB,\n      REGION\n    FROM \"APPLICATION-LOGS\"\n  );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ced04a19-6886-4ca5-91ac-78694accb4ac",
   "metadata": {
    "language": "sql",
    "name": "search_for_authentication_errors"
   },
   "outputs": [],
   "source": "SELECT\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n    'QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.application_logs_search',\n    '{\n      \"query\": \"authentication failed\",\n      \"columns\": [\"MESSAGE\", \"LEVEL\", \"SERVICE\", \"TIMESTAMP\", \"ERROR\"],\n      \"filter\": {\"@eq\": {\"LEVEL\": \"ERROR\"}},\n      \"limit\": 10\n    }'\n  ) as authentication_errors;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7574c350-f276-4c83-8303-cf50d9a39aa9",
   "metadata": {
    "language": "sql",
    "name": "Search_for_payment_issues"
   },
   "outputs": [],
   "source": "SELECT\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n    'QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.application_logs_search',\n    '{\n      \"query\": \"payment declined timeout\",\n      \"columns\": [\"MESSAGE\", \"SERVICE\", \"STATUS_CODE\", \"TIMESTAMP\", \"REQUEST_ID\"],\n      \"filter\": {\"@eq\": {\"SERVICE\": \"payment-service\"}},\n      \"limit\": 10\n    }'\n  );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f501cef2-c27c-444f-9fa6-af50763a3439",
   "metadata": {
    "language": "sql",
    "name": "search_for_db_connection_problems"
   },
   "outputs": [],
   "source": "SELECT\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n    'QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.application_logs_search',\n    '{\n      \"query\": \"database connection timeout\",\n      \"columns\": [\"MESSAGE\", \"LEVEL\", \"SERVICE\", \"HOST\", \"TIMESTAMP\"],\n      \"filter\": {\"@eq\": {\"LEVEL\": \"ERROR\"}},\n      \"limit\": 10\n    }'\n  );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eedc772e-df40-4e9f-a2c0-908eb3f37045",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "### Multiple Filters"
  },
  {
   "cell_type": "code",
   "id": "69d2bb11-34e4-4835-bc35-2edce680d6fc",
   "metadata": {
    "language": "sql",
    "name": "search_payment_or_auth_services"
   },
   "outputs": [],
   "source": "SELECT\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n    'QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.application_logs_search',\n    '{\n      \"query\": \"failed transaction\",\n      \"columns\": [\"MESSAGE\", \"LEVEL\", \"SERVICE\", \"ERROR\", \"STATUS_CODE\", \"TIMESTAMP\"],\n      \"filter\": {\n        \"@and\": [\n          {\"@eq\": {\"LEVEL\": \"ERROR\"}},\n          {\"@or\": [\n            {\"@eq\": {\"SERVICE\": \"payment-service\"}},\n            {\"@eq\": {\"SERVICE\": \"auth-service\"}}\n          ]}\n        ]\n      },\n      \"limit\": 20\n    }'\n  );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "12119e53-9c87-441d-9feb-fc632fba5b58",
   "metadata": {
    "language": "sql",
    "name": "search_system_warnings_with_high_memory_usage"
   },
   "outputs": [],
   "source": "SELECT\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n    'QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.application_logs_search',\n    '{\n      \"query\": \"memory usage warning system resource\",\n      \"columns\": [\"MESSAGE\", \"LEVEL\", \"SERVICE\", \"MEMORY_PERCENT\", \"AVAILABLE_MB\", \"REGION\", \"TIMESTAMP\"],\n      \"filter\": {\n        \"@and\": [\n          {\"@eq\": {\"LEVEL\": \"WARN\"}},\n          {\"@gte\": {\"MEMORY_PERCENT\": 80}}\n        ]\n      },\n      \"limit\": 15\n    }'\n  );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9cd981ff-c14e-4aff-a1fd-6e9b7382e571",
   "metadata": {
    "language": "sql",
    "name": "search_for_disk_space_issues"
   },
   "outputs": [],
   "source": "SELECT\n  SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n    'QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.application_logs_search',\n    '{\n      \"query\": \"disk space running low storage\",\n      \"columns\": [\"MESSAGE\", \"SERVICE\", \"DISK_USAGE_PERCENT\", \"AVAILABLE_GB\", \"REGION\", \"HOST\", \"TIMESTAMP\"],\n      \"filter\": {\n        \"@and\": [\n          {\"@gte\": {\"DISK_USAGE_PERCENT\": 85}},\n          {\"@eq\": {\"REGION\": \"us-west-2\"}}\n        ]\n      },\n      \"limit\": 10\n    }'\n  );",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5dedc016-5177-4b17-87a6-c98427e6f91b",
   "metadata": {
    "name": "Cleanup",
    "collapsed": false
   },
   "source": "## Cleanup\nClean up all Snowflake resources.\n\n> **IMPORTANT**:\n> \n> Stop the Openflow Processors as described in [Chapter 14](https://quickstarts.snowflake.com/guide/getting-started-with-openflow-kafka-connector/index.html?index=..%2F..index#14)\n> Delete the Kafka Topic and Cluster if needed"
  },
  {
   "cell_type": "code",
   "id": "e51d458a-2397-4c04-bd0e-6e726236561d",
   "metadata": {
    "language": "sql",
    "name": "cleanup"
   },
   "outputs": [],
   "source": "-- Use ACCOUNTADMIN to drop objects\nUSE ROLE ACCOUNTADMIN;\n\n-- Drop Cortex Search service (if created)\nDROP CORTEX SEARCH SERVICE IF EXISTS QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.application_logs_search;\n\n-- Drop alerts (if created)\nDROP ALERT IF EXISTS QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.HIGH_ERROR_RATE_ALERT;\nDROP ALERT IF EXISTS QUICKSTART_KAFKA_CONNECTOR_DB.PUBLIC.CRITICAL_SERVICE_ALERT;\n\n-- Drop database (this removes all tables and data)\nDROP DATABASE IF EXISTS QUICKSTART_KAFKA_CONNECTOR_DB;\n\n-- Drop warehouse\nDROP WAREHOUSE IF EXISTS QUICKSTART_KAFKA_CONNECTOR_WH;\n\n-- Drop external access integration\nDROP EXTERNAL ACCESS INTEGRATION IF EXISTS quickstart_kafka_connector_access;\n\n-- Note: We don't drop QUICKSTART_ROLE as it may be used by other quickstarts",
   "execution_count": null
  }
 ]
}